{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read/Write codes\n",
    "\n",
    "\n",
    "def file_reader(source, file_path, header=True, schema=True, multiline=True):\n",
    "  '''\n",
    "  Generic File Reader. Can read any text and binary files and returns the spark dataframe\n",
    "  '''\n",
    "  if source.lower() == \"parquet\":\n",
    "    return spark.read.format(\"parquet\").load(file_path)\n",
    "  elif source.lower() == \"csv\":\n",
    "    return spark.read.format(\"csv\").option(\"header\", header).option(\"inferSchema\", schema) \\\n",
    "                                   .load(file_path)\n",
    "  else:\n",
    "    raise Exception(\"Invalid source type. Please specify a valid source type.\")\n",
    "\n",
    "\n",
    "\n",
    "def file_writer(target, df, file_path, save_mode=\"overwrite\", part_cols=\"\"):\n",
    "  '''\n",
    "  Generic File Writer. Can write any text and binary files\n",
    "  '''\n",
    "  if len(part_cols) > 0:\n",
    "    df.write.format(target).mode(save_mode).partitionBy([col(x) for x in part_cols]) \\\n",
    "                           .save(file_path)\n",
    "  else:\n",
    "    df.write.format(target).mode(save_mode).save(file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge table logic\n",
    "\n",
    "def execute_merge(df, db_params, primary_keys):\n",
    "  '''\n",
    "  Execute the merge statement\n",
    "  '''\n",
    "  try:\n",
    "    src_table = get_delta_table(Constant.SILVER_CONTAINER + \"/\" + db_params.get_database() \\\n",
    "                                                          + \"/\" + db_params.get_table())\n",
    "    src_table.alias(\"a\").merge(df.alias(\"b\"), get_merge_on_columns(primary_keys)) \\\n",
    "                        .whenMatchedUpdateAll() \\\n",
    "                        .whenNotMatchedInsertAll() \\\n",
    "                        .execute()\n",
    "  except Exception:\n",
    "    traceback.print_exc()\n",
    "    raise Exception(\"Error occured when performing merge operation for \" + db_params.get_database() \\\n",
    "                                                                         + \".\" + db_params.get_table())\n",
    "\n",
    "\n",
    "# Casting dataframe columns\n",
    "\n",
    "\n",
    " castDF = curveDF.withColumn(\"date\", to_date(curveDF[\"date\"], \"MM/dd/yyyy\")) \\\n",
    "                        .withColumn(\"cut_off_date\", to_date(curveDF[\"cut_off_date\"], \"MMM-EEE-dd-yyyy\")) \\\n",
    "                        .withColumn(\"peak\", curveDF[\"peak\"].cast(DoubleType())) \\\n",
    "                        .withColumn(\"off_peak\", curveDF[\"off_peak\"].cast(DoubleType())) \\\n",
    "                        .withColumn(\"atc\", curveDF[\"atc\"].cast(DoubleType())) \\\n",
    "                        .withColumnRenamed(\"cut_off_date\", \"short_term_cut_off_date\") \\\n",
    "                        .withColumn(\"effective_date\", lit(ifc.eff_date)) \\\n",
    "                        .withColumn(\"year_diff\", year(\"date\") - year(\"effective_date\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Date conversion logics\n",
    "\n",
    "def columns_to_unix_timestamp(df, table_name, table_attributes):\n",
    "  df = datetime_columns_to_unix_timestamp(df, table_name, table_attributes)\n",
    "  df = datetimeoffset_columns_to_unix_timestamp(df, table_name, table_attributes)\n",
    "  return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def datetime_columns_to_unix_timestamp(df, table_name, table_attributes):\n",
    "  '''\n",
    "  This method converts datetime columns to unix timestamps based on the given table attributes\n",
    "  '''\n",
    "  column_list = table_attributes.get_tablename_to_timestamp_columns().get(table_name)\n",
    "  if column_list:\n",
    "    df = reduce(sql_datetime_to_unix_timestamp, column_list,  df)\n",
    "  return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def datetimeoffset_columns_to_unix_timestamp(df, table_name, table_attributes):\n",
    "  '''\n",
    "  This method converts datetimeoffset columns to unix timestamps based on the given table attributes\n",
    "  '''\n",
    "  column_list = table_attributes.get_tablename_to_datetimeoffset_columns().get(table_name)\n",
    "  if column_list:\n",
    "    df = reduce(sql_datetimeoffset_to_unix_timestamp, column_list,  df)\n",
    "  return df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def sql_datetime_to_unix_timestamp(source_df, column_zoneid_tuple):\n",
    "  '''\n",
    "  Convert datetimes with the supplied timezone to unix timestamps\n",
    "  column_zoneid_tuple - e.g. (\"EnterDate\", \"EST\")\n",
    "  '''\n",
    "  return source_df.withColumn(column_zoneid_tuple[0], to_utc_timestamp(column_zoneid_tuple[0], \n",
    "                                                                             column_zoneid_tuple[1]))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def sql_datetimeoffset_to_unix_timestamp(source_df, column_format_tuple):\n",
    "  '''\n",
    "  Convert datetimeoffset to UTC if you pass in a zone-offset\n",
    "  column_format_tuple - e.g. (\"ModifiedDate\", \"yyyy-MM-dd HH:mm:ss z\")\n",
    "  '''\n",
    "  return source_df.withColumn(column_format_tuple[0], to_timestamp(column_format_tuple[0], \n",
    "                                                                       column_format_tuple[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
